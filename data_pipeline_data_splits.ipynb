{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a4e835-27f8-4758-8f2c-ed47a4582fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os # new code\n",
    "import sys # new code\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import pickle    \n",
    "import signal # new code\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoConfig, AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "from transformers import get_scheduler\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "from sentence_transformers.losses import TripletLoss, MultipleNegativesRankingLoss, GISTEmbedLoss\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50e2c03-9466-49fb-952c-6f565e56319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../../datasets/dataset_training'\n",
    "\n",
    "TRAIN_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_train_08112025.csv' # change the dataset type here\n",
    "EVAL_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_eval_08112025.csv'\n",
    "TEST_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_test_08112025.csv'\n",
    "    \n",
    "\n",
    "def build_huggingface_dataset():\n",
    "    data_files = {\n",
    "        'train': TRAIN_DATASET_PATH,\n",
    "        'eval':  EVAL_DATASET_PATH,\n",
    "        'test':  TEST_DATASET_PATH,\n",
    "    }\n",
    "    # Streaming reads rows lazily from disk\n",
    "    train_dataset = load_dataset('csv', data_files=data_files, split = 'train') #, streaming=True)\n",
    "    eval_dataset = load_dataset('csv', data_files=data_files, split = 'eval')\n",
    "    test_dataset = load_dataset('csv', data_files=data_files, split = 'test')\n",
    "    \n",
    "    # Remove only extraneous columns (if any); retain anchor/positives/negatives\n",
    "    train_dataset = train_dataset.select_columns(['anchor', 'positives', 'negatives'])\n",
    "    eval_dataset = eval_dataset.select_columns(['anchor', 'positives', 'negatives'])\n",
    "    test_dataset = test_dataset.select_columns(['anchor', 'positives', 'negatives'])\n",
    "\n",
    "    model_path = '../../../../shekhar_tanwar/ICD-ICD-Triplet/model/e5-large-v2-20250331143312-finetuned-icd-v30/'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "\n",
    "    def add_triplet_length(batch):\n",
    "        # Concatenate to one long list to tokenize once.\n",
    "        n = len(batch['anchor'])\n",
    "\n",
    "        anchors = [str(x) if x is not None else \"\" for x in batch[\"anchor\"]]\n",
    "        positives = [str(x) if x is not None else \"\" for x in batch[\"positives\"]]\n",
    "        negatives = [str(x) if x is not None else \"\" for x in batch[\"negatives\"]]\n",
    "        \n",
    "        all_texts = anchors + positives + negatives\n",
    "    \n",
    "        # Fast batch tokenization; ask only for lengths\n",
    "        out = tokenizer(\n",
    "            all_texts,\n",
    "            add_special_tokens=False,\n",
    "            return_length=True,\n",
    "            padding=False,\n",
    "            truncation=False\n",
    "        )\n",
    "        lens = np.asarray(out['length'], dtype=np.int32)\n",
    "    \n",
    "        # Slice back by role\n",
    "        anchor_len   = lens[:n]\n",
    "        positive_len = lens[n:2*n]\n",
    "        negative_len = lens[2*n:]\n",
    "    \n",
    "        # Max length across the triplet\n",
    "        triplet_len = np.maximum.reduce([anchor_len, positive_len, negative_len])\n",
    "    \n",
    "        return {'triplet_length': triplet_len.tolist()}\n",
    "\n",
    "\n",
    "    # Choose large batch size; tune based on RAM/CPU cache\n",
    "    BATCHED_SIZE = 8192  # try 8k, 16k, 32k\n",
    "    NUM_PROC = max(2, os.cpu_count() - 2)\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        add_triplet_length,\n",
    "        batched=True,\n",
    "        batch_size=BATCHED_SIZE,\n",
    "        num_proc=NUM_PROC,\n",
    "        remove_columns=[],          # don't drop anything\n",
    "        desc=\"Computing Train triplet lengths\"\n",
    "    )\n",
    "\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        add_triplet_length,\n",
    "        batched=True,\n",
    "        batch_size=BATCHED_SIZE,\n",
    "        num_proc=NUM_PROC,\n",
    "        remove_columns=[],          # don't drop anything\n",
    "        desc=\"Computing Eval triplet lengths\"\n",
    "    )\n",
    "\n",
    "    test_dataset = test_dataset.map(\n",
    "        add_triplet_length,\n",
    "        batched=True,\n",
    "        batch_size=BATCHED_SIZE,\n",
    "        num_proc=NUM_PROC,\n",
    "        remove_columns=[],          # don't drop anything\n",
    "        desc=\"Computing Test triplet lengths\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    return train_dataset, eval_dataset, test_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281c1e83-b87f-4dd2-a086-22e993d72e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 51118300 examples [01:20, 635570.51 examples/s]\n",
      "Generating eval split: 231500 examples [00:01, 154148.57 examples/s]\n",
      "Generating test split: 417100 examples [00:00, 656751.28 examples/s]\n",
      "Computing Train triplet lengths (num_proc=94): 100%|██████████| 51118300/51118300 [01:31<00:00, 558957.82 examples/s] \n",
      "Computing Eval triplet lengths (num_proc=94): 100%|██████████| 231500/231500 [00:01<00:00, 183011.28 examples/s]\n",
      "Computing Test triplet lengths (num_proc=94): 100%|██████████| 417100/417100 [00:01<00:00, 300773.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset, test_dataset = build_huggingface_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72a36d81-274b-4684-9540-a9a9b23003ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 51118300/51118300 [02:20<00:00, 363816.51 examples/s]\n",
      "Filter: 100%|██████████| 51118300/51118300 [02:07<00:00, 401391.29 examples/s]\n",
      "Filter: 100%|██████████| 51118300/51118300 [02:05<00:00, 406623.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def get_batches_by_length(data):\n",
    "    \n",
    "\n",
    "    b0 = data.filter(lambda ex : ex['triplet_length'] <= 64).shuffle(seed = 42)\n",
    "    b1 = data.filter(lambda ex : 64 < ex['triplet_length'] <= 128).shuffle(seed = 42)\n",
    "    b2 = data.filter(lambda ex : 128 < ex['triplet_length']).shuffle(seed = 42)\n",
    "\n",
    "    merged_data = concatenate_datasets([b0, b1, b2])\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "train_dataset =  get_batches_by_length(train_dataset)\n",
    "eval_dataset =  get_batches_by_length(eval_dataset)\n",
    "test_dataset =  get_batches_by_length(test_dataset)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0b79ba-d05e-48da-a303-b1e7867b4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (14/14 shards): 100%|██████████| 51118300/51118300 [01:16<00:00, 667699.83 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 231500/231500 [00:00<00:00, 534506.25 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 417100/417100 [00:00<00:00, 560993.86 examples/s] \n"
     ]
    }
   ],
   "source": [
    "output_dir = '../../../datasets/dataset_training/triplet_v50_splits'\n",
    "\n",
    "train_dataset.save_to_disk(f'{output_dir}/train')\n",
    "eval_dataset.save_to_disk(f'{output_dir}/eval')\n",
    "test_dataset.save_to_disk(f'{output_dir}/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a145a6d-8f81-44eb-bd35-f96145427ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape : (51119000, 4)\n",
      "eval data shape : (231500, 4)\n",
      "test data shape : (417100, 4)\n",
      "train data shape : (51118300, 4)\n",
      "eval data shape : (231500, 4)\n",
      "test data shape : (417100, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../../../datasets/dataset_training'\n",
    "\n",
    "TRAIN_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_train_08112025.csv' # change the dataset type here\n",
    "EVAL_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_eval_08112025.csv'\n",
    "TEST_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_test_08112025.csv'\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_DATASET_PATH).iloc[:,1:]\n",
    "eval_data = pd.read_csv(EVAL_DATASET_PATH).iloc[:,1:]\n",
    "test_data = pd.read_csv(TEST_DATASET_PATH).iloc[:,1:]\n",
    "\n",
    "print(f'train data shape : {train_data.shape}')\n",
    "print(f'eval data shape : {eval_data.shape}')\n",
    "print(f'test data shape : {test_data.shape}')\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "eval_data = eval_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "print(f'train data shape : {train_data.shape}')\n",
    "print(f'eval data shape : {eval_data.shape}')\n",
    "print(f'test data shape : {test_data.shape}')\n",
    "\n",
    "train_data.to_csv(TRAIN_DATASET_PATH)\n",
    "eval_data.to_csv(EVAL_DATASET_PATH)\n",
    "test_data.to_csv(TEST_DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ac7970-f30f-4fe3-b161-9756621199b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset, eval_dataset, test_dataset = build_huggingface_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ca5c3-f498-4010-831c-ca67c9786692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1763f73a-9399-44f9-922c-e8d6412266f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape : (51118300, 4)\n",
      "eval data shape : (231500, 4)\n",
      "test data shape : (417100, 4)\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../../datasets/dataset_training'\n",
    "\n",
    "TRAIN_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_train_08112025.csv' # change the dataset type here\n",
    "EVAL_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_eval_08112025.csv'\n",
    "TEST_DATASET_PATH = f'{file_path}/triplet_dataset_v50_250_queries_10positives_50hn_test_08112025.csv'\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_DATASET_PATH).iloc[:,1:]\n",
    "eval_data = pd.read_csv(EVAL_DATASET_PATH).iloc[:,1:]\n",
    "test_data = pd.read_csv(TEST_DATASET_PATH).iloc[:,1:]\n",
    "\n",
    "print(f'train data shape : {train_data.shape}')\n",
    "print(f'eval data shape : {eval_data.shape}')\n",
    "print(f'test data shape : {test_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96fa4aff-a5db-4187-b4c4-b8ef31ecdbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specialty</th>\n",
       "      <th>anchor</th>\n",
       "      <th>positives</th>\n",
       "      <th>negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acupuncturist_acupuncturist</td>\n",
       "      <td>acupuncture for swelling</td>\n",
       "      <td>Pain in right lower limb NOS</td>\n",
       "      <td>Abscess of bursa, wrist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acupuncturist_acupuncturist</td>\n",
       "      <td>acupuncture for swelling</td>\n",
       "      <td>Pain in right lower limb NOS</td>\n",
       "      <td>Effusion, unspecified hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acupuncturist_acupuncturist</td>\n",
       "      <td>acupuncture for swelling</td>\n",
       "      <td>Pain in right lower limb NOS</td>\n",
       "      <td>Whitlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acupuncturist_acupuncturist</td>\n",
       "      <td>acupuncture for swelling</td>\n",
       "      <td>Pain in right lower limb NOS</td>\n",
       "      <td>Other acne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acupuncturist_acupuncturist</td>\n",
       "      <td>acupuncture for swelling</td>\n",
       "      <td>Pain in right lower limb NOS</td>\n",
       "      <td>Localized swelling, mass and lump, neck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     specialty                    anchor  \\\n",
       "0  acupuncturist_acupuncturist  acupuncture for swelling   \n",
       "1  acupuncturist_acupuncturist  acupuncture for swelling   \n",
       "2  acupuncturist_acupuncturist  acupuncture for swelling   \n",
       "3  acupuncturist_acupuncturist  acupuncture for swelling   \n",
       "4  acupuncturist_acupuncturist  acupuncture for swelling   \n",
       "\n",
       "                      positives                                negatives  \n",
       "0  Pain in right lower limb NOS                  Abscess of bursa, wrist  \n",
       "1  Pain in right lower limb NOS               Effusion, unspecified hand  \n",
       "2  Pain in right lower limb NOS                                  Whitlow  \n",
       "3  Pain in right lower limb NOS                               Other acne  \n",
       "4  Pain in right lower limb NOS  Localized swelling, mass and lump, neck  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f388ee1-59e9-41e7-b3d7-1a0f69742b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c845922-9e0e-425a-b3bc-8d02907ef9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51118300, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c6413-616c-4e51-961e-bde314004244",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76999628-e293-4e7e-864c-236c2975578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import pickle\n",
    "import warnings\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from azureml.core import Workspace\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import  PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser, JsonOutputKeyToolsParser, CommaSeparatedListOutputParser\n",
    "pd.set_option('display.max_rows', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49574e-6ed1-476a-898e-0dc7025ca552",
   "metadata": {},
   "source": [
    "# COMBINING LIST OF DICTIONARY INTO SINGLE DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdfc23e-8ef9-44a6-87e0-4cfb1a65e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_splits_chunks(file_path : str, output_file_path : str):\n",
    "    \n",
    "    all_files = [file_path + file for file in os.listdir(file_path) if '.json' in file]\n",
    "\n",
    "    all_split_files_dict = []\n",
    "    for file_path in all_files:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        all_split_files_dict.append(data)\n",
    "\n",
    "\n",
    "    \n",
    "    # COMBINING LIST OF DICTIONARY INTO SINGLE DICTIONARY AND FILTER OUT QUERIES WITH NO ICD CODES\n",
    "    specialty_query_codes_dict = {}\n",
    "    for data in all_split_files_dict:\n",
    "        specialty = list(data.keys())[0]\n",
    "        query_codes_dict = list(data.values())[0]\n",
    "    \n",
    "        query_codes = {}\n",
    "        for query, codes in query_codes_dict.items():\n",
    "            if len(codes) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                query_codes[query] = codes\n",
    "    \n",
    "        specialty_query_codes_dict[specialty] = query_codes\n",
    "\n",
    "\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        json.dump(specialty_query_codes_dict, file, indent = 4)\n",
    "\n",
    "    return all_split_files_dict , specialty_query_codes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66915445-7c2c-4049-882a-d9f5bde35795",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../../datasets/datasets_augmented/final_dataset_v40/icd_filtered/filtered_icd_codes/'\n",
    "\n",
    "output_file_path = '../../../datasets/datasets_augmented/final_dataset_v40/specialty_verification/filtered_specialty_query_dict.json'\n",
    "all_split_files_dict , specialty_query_codes_dict = load_splits_chunks(file_path = file_path, output_file_path = output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c7038e-4240-42d4-9f7c-4bbdedabe2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_specialties = list(specialty_query_codes_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298ee5c-6f2d-41bb-aed6-6341c736a93a",
   "metadata": {},
   "source": [
    "# CREATING SPLITS FOR FILTERED DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ed4824-81db-4c5d-a399-9bff6d9dffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_input_data(input_file : str, output_dir : str, num_chunks : int):\n",
    "    \n",
    "    splits_dir = f\"{output_dir}splits/\"\n",
    "    all_file_paths = [splits_dir + file for file in splits_dir if 'json' in file]\n",
    "    if len(all_file_paths) == num_chunks:\n",
    "        all_split_files = []\n",
    "        \n",
    "        for input_file in all_file_paths:\n",
    "            with open(input_file, 'r') as f:\n",
    "                specialty_data = json.load(f)\n",
    "            \n",
    "            all_split_files.append(specialty_data)\n",
    "            \n",
    "    else:\n",
    "    \n",
    "        # Create output directory\n",
    "        splits_dir = f\"{output_dir}splits/\"\n",
    "        os.makedirs(splits_dir, exist_ok=True)\n",
    "\n",
    "        # Load input data\n",
    "        with open(input_file, 'r') as f:\n",
    "            specialty_data = json.load(f)\n",
    "\n",
    "        # Get list of specialties\n",
    "        specialties = list(specialty_data.keys())\n",
    "        chunk_size = len(specialties) // num_chunks + (1 if len(specialties) % num_chunks else 0)\n",
    "\n",
    "        all_split_files = []\n",
    "        # Create chunks\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, len(specialties))\n",
    "\n",
    "            chunk_specialties = specialties[start_idx:end_idx]\n",
    "            chunk_data = {specialty: specialty_data[specialty] for specialty in chunk_specialties}\n",
    "\n",
    "            # Save chunk\n",
    "            filename = input_file.split('/')[-1].split('.')[0]\n",
    "            chunk_file = f\"{splits_dir}{filename}_split_{i}.json\"\n",
    "            all_split_files.append(chunk_data)\n",
    "            with open(chunk_file, 'w') as f:\n",
    "                json.dump(chunk_data, f, indent=4)\n",
    "\n",
    "            #print(f\"Chunk {i}: {len(chunk_specialties)} specialties, saved to {chunk_file}\")\n",
    "\n",
    "    return all_split_files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fdc463-12d7-48be-a86d-f23bd6f9976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Split for specialty_query_dict\n"
     ]
    }
   ],
   "source": [
    "input_file_filtered_specialty_query_code_dict = '../../../datasets/datasets_augmented/final_dataset_v40/specialty_verification/filtered_specialty_query_dict.json'\n",
    "\n",
    "output_dir_filtered_specialty_query_dict = '../../../datasets/datasets_augmented/final_dataset_v40/specialty_verification/filtered_specialty_query_splits/'\n",
    "\n",
    "num_chunks = 4\n",
    "print(f'Creating Split for specialty_query_dict')\n",
    "all_split_files_specialty_query_code_dict = split_input_data(input_file = input_file_filtered_specialty_query_code_dict, output_dir = output_dir_filtered_specialty_query_dict , num_chunks = num_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0591c2-d191-468c-82ea-1d7044dada61",
   "metadata": {},
   "source": [
    "# CREATING SPECIALTY QUERY CODE DESCRIPTION DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1bfd7ba-44ca-4677-a76b-7125ddfd10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icd_dataset(icd_reference_file : str):\n",
    "    icd_reference_file = '../../../../shekhar_tanwar/ICD-ICD-Triplet/dataset/icd10.csv'\n",
    "    dataset_icd = pd.read_csv(icd_reference_file).iloc[:,1:]\n",
    "    \n",
    "    dataset_icd = dataset_icd.drop_duplicates()\n",
    "    dataset_icd = dataset_icd.iloc[:,13:15]\n",
    "    dataset_icd.columns = ['ICD_Codes','Description']\n",
    "    dataset_icd['ICD_Codes'] = dataset_icd['ICD_Codes'].apply(lambda x : x.strip())\n",
    "    dataset_icd['Description'] = dataset_icd['Description'].apply(lambda x : x.strip())\n",
    "    dataset_icd = dataset_icd.drop_duplicates(subset = ['ICD_Codes'], keep = 'first')\n",
    "\n",
    "    icd_reference_lookup = {}\n",
    "\n",
    "    for index, row in dataset_icd.iterrows():\n",
    "        icd_reference_lookup[row.ICD_Codes] = row.Description\n",
    "\n",
    "    return dataset_icd, icd_reference_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f359e1-fc83-40f7-ba6e-fecde81762cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_reference_file = '../../../../shekhar_tanwar/ICD-ICD-Triplet/dataset/icd10.csv'    \n",
    "dataset_icd, icd_reference_lookup = get_icd_dataset(icd_reference_file = icd_reference_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f5cd26-51f5-4689-99d3-9cf791cad70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_query_icd_code_description_dataset(icd_reference_lookup : dict,  specialty_query_codes_dict : dict):\n",
    "    \n",
    "    # path to the filtered specialt query code dict files\n",
    "    # file_path = '../../../datasets/datasets_augmented/final_dataset_v40/icd_filtered/filtered_icd_codes/'\n",
    "    # read all the files in the file_path\n",
    "\n",
    "\n",
    "    specialties = list(specialty_query_codes_dict.keys())\n",
    "    specialty_query_code_desciption_dict = {}\n",
    "    problematic_specialty_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(specialties))):\n",
    "        medical_specialty_subspecialty = specialties[i]\n",
    "\n",
    "        try:\n",
    "            query_codes_dict = specialty_query_codes_dict.get(medical_specialty_subspecialty)\n",
    "            query_code_description_dict = {}\n",
    "            \n",
    "            \n",
    "            for medical_query, retrieved_codes_gpt41 in query_codes_dict.items():\n",
    "                icd_code_description_list = []\n",
    "                # final selected_codes\n",
    "                for code in retrieved_codes_gpt41:\n",
    "                    if code in icd_reference_lookup:\n",
    "                        icd_code_description_list.append(code + \" : \" + icd_reference_lookup.get(code))\n",
    "        \n",
    "                query_code_description_dict[medical_query] = icd_code_description_list\n",
    "    \n",
    "            specialty_query_code_desciption_dict[medical_specialty_subspecialty] = query_code_description_dict\n",
    "\n",
    "        except:\n",
    "            problematic_specialty_list.append(specialty)\n",
    "    \n",
    "    with open('../../../datasets/datasets_augmented/final_dataset_v40/icd_filtered/specialty_query_code_desciption_splits/filtered_specialty_query_code_desciption_dict.json', 'w') as file:\n",
    "       json.dump(specialty_query_code_desciption_dict, file, indent = 4)             \n",
    "\n",
    "    return specialty_query_code_desciption_dict, problematic_specialty_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a8f6f4-7062-462e-ab0f-4f37efb6074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 591/591 [00:01<00:00, 511.17it/s]\n"
     ]
    }
   ],
   "source": [
    "specialty_query_code_desciption_dict, _  = get_query_icd_code_description_dataset(icd_reference_lookup = icd_reference_lookup,  specialty_query_codes_dict = specialty_query_codes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f33442-c422-4a22-8d59-20ed39b6a898",
   "metadata": {},
   "source": [
    "# SPECIALTY VERIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57ffdec2-2e0b-45cf-b6b1-4e5fe82a665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BearerAuth(requests.auth.AuthBase):\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "    def __call__(self, r):\n",
    "        r.headers[\"authorization\"] = \"Bearer \" + self.token\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be3eb213-1443-4a50-b068-ae5bf487984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm(model_name) -> AzureChatOpenAI:\n",
    "    ws = Workspace.from_config()\n",
    "    keyvault = ws.get_default_keyvault()\n",
    "    credential = DefaultAzureCredential()\n",
    "    workspacename = keyvault.get_secret(\"project-workspace-name\")\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    os.environ[\"AZURE_OPENAI_KEY\"] = access_token.token\n",
    "    openai.api_type = \"azure_ad\"\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = f\"https://{workspacename}openai.openai.azure.com/\"\n",
    "    openai.api_version = \"2023-07-01-preview\"\n",
    "    subscriptionId = keyvault.get_secret(\"project-subscription-id\")\n",
    "    # Ensure you have these environment variables set up with your Azure OpenAI credentials\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = \"ee0dd46654bd4427ba4f5580b5a0db0a\"\n",
    "    os.environ[\"AZURE_OPENAI_API_BASE\"] = \"https://xqrojjmb2wjlqopopenai.openai.azure.com/\"\n",
    "\n",
    "    if model_name == \"gpt-4o\":\n",
    "        os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
    "        os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "    \n",
    "        subscriptionId = keyvault.get_secret(\"project-subscription-id\")\n",
    "        apiVersion = \"2023-10-01-preview\"\n",
    "        url = f\"https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{workspacename}-common/providers/Microsoft.CognitiveServices/accounts/{workspacename}openai/deployments?api-version={apiVersion}\"\n",
    "        accessToken = credential.get_token(\"https://management.azure.com/.default\")\n",
    "        response = requests.get(url, auth=BearerAuth(accessToken.token))\n",
    "        print(f'Initializing Model : {os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]}')\n",
    "        model = AzureChatOpenAI(\n",
    "                    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "                    azure_endpoint=os.environ[\"AZURE_OPENAI_API_BASE\"],\n",
    "                    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "                    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0.9,\n",
    "                    model_kwargs={\"seed\": 1337}\n",
    "                )\n",
    "        \n",
    "        print(f'Model {model_name} Initialized')\n",
    "\n",
    "    elif model_name == \"gpt-4.1\":\n",
    "\n",
    "        os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-12-01-preview\"\n",
    "        os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4.1\"\n",
    "    \n",
    "    \n",
    "        subscriptionId = keyvault.get_secret(\"project-subscription-id\")\n",
    "        apiVersion = \"2024-12-01-preview\"\n",
    "        url = f\"https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{workspacename}-common/providers/Microsoft.CognitiveServices/accounts/{workspacename}openai/deployments?api-version={apiVersion}\"\n",
    "        accessToken = credential.get_token(\"https://management.azure.com/.default\")\n",
    "        response = requests.get(url, auth=BearerAuth(accessToken.token));\n",
    "    \n",
    "        print(f'Initializing Model : {os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]}')\n",
    "        model = AzureChatOpenAI(\n",
    "                    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "                    azure_endpoint=os.environ[\"AZURE_OPENAI_API_BASE\"],\n",
    "                    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "                    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0.9,\n",
    "                    model_kwargs={\"seed\": 1337}\n",
    "                )\n",
    "        \n",
    "        print(f'Model {model_name} Initialized')\n",
    "\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83fa7a8b-3426-4660-94bf-652c1e822d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specialty_verification(model : AzureChatOpenAI, medical_specialty_subspecialty : str, medical_query : str, icd_code_description_list : list):\n",
    "\n",
    "\n",
    "    class SpecialtiesResponse(BaseModel):\n",
    "        queries: List[str] = Field(description=\"List of queries corresponding to user provided medical specialty\")\n",
    "\n",
    "    # Set up the PydanticOutputParser with the SpecialtiesResponse model\n",
    "    output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "\n",
    "    system_prompt = \"\"\" You are a certified medical coder who assigns ICD-10 codes.\n",
    "\n",
    "            **Goal:**  \n",
    "            Given (1) a medical search query, (2) a user-supplied list of **ICD-10 code: description** pairs, and (3) a reference medical **specialty_subspecialty**, identify whether the reference **specialty_subspecialty** is **relevant** or **non-relevant** to the medical query and the ICD-10 code-description list. The result should be **non-relevant** if the reference **specialty_subspecialty** is either too generic for the query/ICD-10 list’s intent or clearly unrelated to that query and code list.\n",
    "            \n",
    "            **How to decide relevance:**  \n",
    "            - **Understand the query’s clinical intent:** Determine what condition, symptom, or scenario the query is describing.  \n",
    "            - **Consider the ICD-10 code list context:** The ICD-10 codes and descriptions are chosen to be consistent with each other and with the query, representing a specific clinical scenario. Use this combined context to inform your decision.  \n",
    "            - **Match with the specialty_subspecialty:** Evaluate whether a practitioner of the reference **specialty_subspecialty** typically addresses the query’s scenario:  \n",
    "              - The specialty_subspecialty pair may consist of a broad specialty and a more focused subspecialty. Emphasize the general **specialty** domain. If the scenario falls under that general field (even if not an exact subspecialty match), it can be considered relevant.  \n",
    "              - If the scenario (query + codes) **does not fall under** the domain of the reference specialty_subspecialty — for example, the specialty_subspecialty is overly broad/vague for this specific case, or it pertains to a different field of medicine — then label it **non-relevant**.  \n",
    "              - If the scenario **does** fall under the clinical domain of that specialty_subspecialty (i.e. a provider of that type would reasonably handle such cases), then label it **relevant**.  \n",
    "            - **Multiple possible specialties:** There may be cases where the query and codes could belong to more than one specialty. You are **only** checking the given reference specialty_subspecialty. If the given specialty_subspecialty is one appropriate choice for this scenario, mark it **relevant** (even if other specialties could also be involved).  \n",
    "            - **If unsure:** If you cannot confidently determine relevance from the information provided, label the result as `CANNOT_DECIDE`.\n",
    "            \n",
    "            **Response format (strict):**  \n",
    "            Return **only** a single label as the answer: `relevant`, `non-relevant`, or `CANNOT_DECIDE` (use `CANNOT_DECIDE` only if you truly cannot decide). Do **not** include explanations, reasoning, or any additional text.\n",
    "            \n",
    "            **Inputs (to be inserted at runtime):**  \n",
    "            medical_query: *{medical_query}*  \n",
    "            icd_code_description_list: *{icd_code_description_list}*  \n",
    "            medical_specialty_subspecialty: *{medical_specialty_subspecialty}*  \n",
    "            \n",
    "            **Few-shot guidance (examples):**\n",
    "            \n",
    "            Example 1:  \n",
    "            medical_query: **aging and decreased independence and mobility**  \n",
    "            icd_code_description_list: **['Z74.3 : Need for continuous supervision', 'Z73.89 : Other problems related to life management difficulty', 'Z73.6 : Limitation of activities due to disability', 'Z60.0 : Phase of life problem', 'Z74.2 : Need for assistance at home and no other household member able to render care', 'Z74.1 : Need for assistance with personal care', 'Z74.09 : Other reduced mobility', 'R54 : Senile debility', 'Z91.81 : History of falling']**  \n",
    "            medical_specialty_subspecialty: **adult companion_adult companion**  \n",
    "            Expected output: **relevant**\n",
    "            \n",
    "            Example 2:  \n",
    "            medical_query: **acupuncture for headaches**  \n",
    "            icd_code_description_list: **['R51.9 : Headache, unspecified', 'G44.209 : Tension-type headache, unspecified, not intractable', 'G43.009 : Migraine without aura NOS', 'G44.89 : Other headache syndrome', 'G43.909 : Migraine NOS', 'G43.709 : Chronic migraine without aura NOS']**  \n",
    "            medical_specialty_subspecialty: **anesthesiology_addiction medicine**  \n",
    "            Expected output: *non-relevant**\n",
    "            \n",
    "            Example 3:  \n",
    "            medical_query: **specialty biologic and injectable therapies in healthcarer**  \n",
    "            icd_code_description_list: **['T88.59XA : Other complications of anesthesia, initial encounter', 'T41.1X5A : Adverse effect of intravenous anesthetics, initial encounter', 'T88.7XXA : Unspecified adverse effect of drug or medicament, initial encounter']**  \n",
    "            medical_specialty_subspecialty: **cliniccenter_student health**  \n",
    "            Expected output: **non-relevant**\n",
    "            \n",
    "            Example 4:  \n",
    "            medical_query: **itchy scalp after workplace exposure**  \n",
    "            icd_code_description_list: **['L23.9 : Allergic contact dermatitis, unspecified cause', 'L28.0 : Circumscribed neurodermatitis', 'L25.9 : Unspecified contact dermatitis, unspecified cause', 'L23.8 : Allergic contact dermatitis due to other agents', 'L23.5 : Allergic contact dermatitis due to plastic', 'L29.8 : Other pruritus', 'R21 : Rash and other nonspecific skin eruption', 'L50.9 : Urticaria, unspecified', 'L24.9 : Irritant contact dermatitis, unspecified cause', 'L27.2 : Dermatitis due to ingested food', 'L24.0 : Irritant contact dermatitis due to detergents']**  \n",
    "            medical_specialty_subspecialty: **cardiologist**  \n",
    "            Expected output: **non-relevant**\n",
    "            \n",
    "            Example 5:  \n",
    "            medical_query: **acne worsening at job**  \n",
    "            icd_code_description_list: **['L70.1 : Acne conglobata', 'L21.9 : Seborrheic dermatitis, unspecified', 'L30.9 : Eczema NOS', 'L25.9 : Unspecified contact dermatitis, unspecified cause', 'L70.0 : Acne vulgaris', 'L23.5 : Allergic contact dermatitis due to plastic', 'L71.9 : Rosacea, unspecified', 'L24.9 : Irritant contact dermatitis, unspecified cause', 'L70.8 : Other acne', 'L70.9 : Acne, unspecified', 'L24.0 : Irritant contact dermatitis due to detergents', 'L71.0 : Perioral dermatitis']**  \n",
    "            medical_specialty_subspecialty: **dermatopathology_occupational medicine**  \n",
    "            Expected output: **relevant**\n",
    "\n",
    "            Example 5:  \n",
    "            medical_query: **how does diabetes affect brain tumour**  \n",
    "            icd_code_description_list: **['L70.1 : Acne conglobata', 'L21.9 : Seborrheic dermatitis, unspecified', 'L30.9 : Eczema NOS', 'L25.9 : Unspecified contact dermatitis, unspecified cause', 'L70.0 : Acne vulgaris', 'L23.5 : Allergic contact dermatitis due to plastic', 'L71.9 : Rosacea, unspecified', 'L24.9 : Irritant contact dermatitis, unspecified cause', 'L70.8 : Other acne', 'L70.9 : Acne, unspecified', 'L24.0 : Irritant contact dermatitis due to detergents', 'L71.0 : Perioral dermatitis']**  \n",
    "            medical_specialty_subspecialty: **neurology**  \n",
    "            Expected output: **CANNOT_DECIDE**\n",
    "            \n",
    "            \n",
    "            **Remember:** Provide **only** the label (`relevant`, `non-relevant`, or `CANNOT_DECIDE`) as the answer. Do not add any explanation or extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        template=system_prompt\n",
    "    )\n",
    "    \n",
    "    \n",
    "    chain = prompt_template | model | output_parser\n",
    "    result = chain.invoke(input={\"medical_specialty_subspecialty\": medical_specialty_subspecialty, \"medical_query\": medical_query,\"icd_code_description_list\" : icd_code_description_list, \"format_instructions\" : output_parser.get_format_instructions()})\n",
    "    return result    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72d246d3-b02f-4fda-a756-292fe0022476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model : gpt-4.1\n",
      "Model gpt-4.1 Initialized\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt-4.1\"\n",
    "model = initialize_llm(model_name = model_name)\n",
    "#retry_dict = get_icd_code_processor(model_name = model_name, chunk_specialty_query_code_dict = chunk_specialty_query_code_dict, chunk_specialty_query_code_description_dict = chunk_specialty_query_code_description_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5e668-4bf2-41e1-9ed7-f5daaf90ad14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04e5619c-a49d-443b-a538-4b1e1d8ced0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specialty_subspecialty : registered nurse_ophthalmic\n",
      "query : how does diabetes affect vision\n",
      "icd_code_description_list : ['E11.339 : Type 2 diabetes mellitus with moderate nonproliferative diabetic retinopathy without macular edema', 'H43.10 : Vitreous hemorrhage, unspecified eye', 'E11.319 : Type 2 diabetes w unsp diabetic rtnop w/o macular edema', 'E11.36 : Type 2 diabetes mellitus with diabetic cataract', 'E11.349 : Type 2 diab w severe nonprlf diab rtnop w/o macular edema', 'H35.0 : Background retinopathy and retinal vascular changes', 'E08.311 : Diabetes mellitus due to underlying condition with unspecified diabetic retinopathy with macular edema', 'E13.311 : Other specified diabetes mellitus with unspecified diabetic retinopathy with macular edema', 'E11.329 : Type 2 diabetes mellitus with mild nonproliferative diabetic retinopathy without macular edema', 'E11.39 : Type 2 diabetes mellitus with other diabetic ophthalmic complication', 'E11.311 : Type 2 diabetes w unsp diabetic retinopathy w macular edema', 'E11.359 : Type 2 diabetes mellitus with proliferative diabetic retinopathy without macular edema']\n"
     ]
    }
   ],
   "source": [
    "reference_medical_specialty_subspecialty = list(specialty_query_code_desciption_dict.keys())[540]\n",
    "medical_query = list(specialty_query_code_desciption_dict.get(reference_medical_specialty_subspecialty))[6]\n",
    "icd_code_description_list = specialty_query_code_desciption_dict.get(reference_medical_specialty_subspecialty).get(medical_query)\n",
    "\n",
    "print(f'specialty_subspecialty : {reference_medical_specialty_subspecialty}')\n",
    "print(f'query : {medical_query}')\n",
    "print(f'icd_code_description_list : {icd_code_description_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b998602e-a4b4-4b15-9b07-7ca79bf4611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_specialty_verification(model = model, medical_specialty_subspecialty = reference_medical_specialty_subspecialty, medical_query = medical_query, icd_code_description_list = icd_code_description_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de9bdab8-c18f-4137-a87a-cabc2c3d600f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relevant']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e0e85-c3c0-4af7-8702-7c51f6a1ed04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
